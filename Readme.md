# VIDEO_GEINE
# ğŸ® AI Video Editor - Turn Audio or Text into AI Videos!

This is a command-line based AI video editor that converts your **voice or story text** into a full **AI-generated video**, complete with translated captions, artwork-style visuals, and music. It combines the power of **Whisper**, **Deep Translator**, **Stable Diffusion**, and **MoviePy** into one streamlined pipeline.

> âš¡ First version built as a resume project â€” future versions will include frontend UI!

---

## ğŸŒŸ Features

* ğŸ¤ Accept audio (mic or file) or user-pasted story text
* ğŸ§  Transcribe using OpenAI Whisper and translate to English
* ğŸ¨ Generate image prompts using custom styles + genres
* ğŸ–¼ï¸ Use Google Colab to generate AI images (Stable Diffusion XL)
* ğŸ® Automatically generate a final video with:

  * Image transitions (fade, crossfade, zoom, etc.)
  * Text overlays (optional, top/bottom)
  * Background audio synced to timing

---

## ğŸ§± Tech Stack

| Area              | Tools Used                                                   |
| ----------------- | ------------------------------------------------------------ |
| Speech to Text    | [Whisper](https://github.com/openai/whisper)                 |
| Translation       | [Deep Translator](https://pypi.org/project/deep-translator/) |
| Prompt Generation | Python (custom logic + formatting)                           |
| Image Generation  | Stable Diffusion via Google Colab                            |
| Video Creation    | [MoviePy](https://zulko.github.io/moviepy/)                  |
| Text on Images    | Pillow (instead of ImageMagick)                              |
| Interface         | Python CLI (`main.py`)                                       |

---

## ğŸ“ Folder Structure

```
project/
â”œâ”€â”€ main.py
â”œâ”€â”€ scripts/            # was locked in gihub and under development 
â”‚   â”œâ”€â”€ generate_audio.py                       
â”‚   â”œâ”€â”€ speech_to_text.py
â”‚   â”œâ”€â”€ save_story.py
â”‚   â”œâ”€â”€ text_to_image_prompt.py
â”‚   â”œâ”€â”€ combine_results.py
â”‚   â”œâ”€â”€ generate_video.py
â”‚   â””â”€â”€ unzip.py
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ story/
â”‚   â”œâ”€â”€ audio/
â”‚   â”‚   â”œâ”€â”€ input/
â”‚   â”‚   â””â”€â”€ output/
â”‚   â”œâ”€â”€ transcripts/
â”‚   â”œâ”€â”€ prompts/
â”‚   â”œâ”€â”€ images/
â”‚   â”œâ”€â”€ final/
â”‚   â””â”€â”€ colab notebook/
â”‚       â””â”€â”€ generate_images.ipynb
```

---

## ğŸš€ How to Use

1. Clone the repo:

   ```
   git clone https://github.com/srihari1105/ai-video-editor.git
   cd ai-video-editor
   ```

2. Set up the virtual environment:

   ```
   python -m venv .venv
   .venv\Scripts\activate
   pip install -r requirements.txt
   ```

3. Run the project:

   ```
   python main.py
   ```

4. Follow the guided CLI. Choose whether you want to:

   * Use mic/file for audio
   * Select styles and genre
   * Add captions, transition type
   * Upload to Colab and generate AI images
   * Get a final video in `/data/final_video.mp4`

---

## ğŸ¤¯ Lessons & Struggles

| Challenge                               | Solution                                                |
| --------------------------------------- | ------------------------------------------------------- |
| Whisper errors due to Python version    | Switched to Python 3.10.11 for compatibility            |
| ImageMagick blocked text rendering      | Replaced with Pillow-based overlay                      |
| Terminal screen looked messy            | Added spacing, emoji icons, and `time.sleep()`          |
| Image generation without GPU            | Used Colab backend with `.ipynb` upload                 |
| Script-to-script integration in Python  | Refactored everything to use **function-based modules** |
| Need to test stories of different types | Built action, horror, mythology stories for demo        |

---

## ğŸ§ª Example Output

* ğŸ“– Input: Mythological Story (typed or spoken)
* ğŸ¤ Audio: Converted using gTTS or user input
* ğŸ–¼ï¸ Images: Created via Stable Diffusion prompts
* ğŸï¸ Final Output: Smooth slideshow video with text overlay + sound

> Final video path: `data/final_video.mp4`

---

## ğŸ› ï¸ Future Plans

* ğŸ”Œ Add a **Gradio** or **Lovable AI** frontend
* ğŸ’¡ Add support for background music selection
* â˜ï¸ Move image generation from Colab to local GPU (or cloud API)
* ğŸ”— Deploy as a web app for public use

---

## ğŸ¤ Acknowledgements

Thanks to:

* [OpenAI Whisper](https://github.com/openai/whisper)
* [Deep Translator](https://pypi.org/project/deep-translator/)
* [MoviePy](https://zulko.github.io/moviepy/)
* [Google Colab](https://colab.research.google.com)

---

## ğŸ”— Connect

This was my **first end-to-end AI project** â€” built with passion, learning, and a lot of debugging ğŸ’¥

ğŸ“¬ Feel free to connect with me on [LinkedIn](https://www.linkedin.com/in/srihari-singupurapu)
ğŸ¥ Demo video, docs, and UI will come in the next version!

---


ğŸš« Note: Core project logic (e.g., image generation, audio processing, etc.) has been removed from public access to prevent plagiarism.

This project is shared only for **portfolio demonstration** purposes. Please do not clone and claim as your own.
